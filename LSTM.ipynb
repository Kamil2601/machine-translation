{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Define common date formats\n",
    "date_formats = [\n",
    "    \"%d.%m.%Y\",\n",
    "    \"%B %d, %Y\",\n",
    "    \"%b %d, %Y\",\n",
    "    \"%d/%m/%Y\",\n",
    "]\n",
    "\n",
    "# Generate random dates in various formats\n",
    "def generate_dates(n):\n",
    "    dates = []\n",
    "    for _ in range(n):\n",
    "        date = fake.date_object()\n",
    "        date_str = date.strftime(random.choice(date_formats))\n",
    "        target_str = date.strftime(\"%Y-%m-%d\")\n",
    "        dates.append((date_str, target_str))\n",
    "    return dates\n",
    "\n",
    "# Generate 10000 date samples\n",
    "dates = generate_dates(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class DateTokenizer:\n",
    "    def __init__(self, dates):\n",
    "        self.pad_token = \"[PAD]\"\n",
    "        self.unk_token = \"[UNK]\"\n",
    "        self.bos_token = \"[BOS]\"\n",
    "        self.eos_token = \"[EOS]\"\n",
    "        self.pad_id = 0\n",
    "        self.unk_id = 1\n",
    "        self.bos_id = 2\n",
    "        self.eos_id = 3\n",
    "\n",
    "        self.special_tokens_id = [self.pad_id, self.unk_id, self.bos_id, self.eos_id]\n",
    "        self.special_tokens = [self.pad_token, self.unk_token, self.bos_token, self.eos_token]\n",
    "\n",
    "        self.tokens = self.special_tokens + sorted(list(set(\"\".join([d[0] for d in dates] + [d[1] for d in dates]))))\n",
    "\n",
    "        self.char2idx = {char: idx for idx, char in enumerate(self.tokens)}\n",
    "        self.idx2char = {idx: char for idx, char in enumerate(self.tokens)}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return [self.bos_token] + [char if (char in self.tokens) else self.unk_token for char in text] + [self.eos_token]\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return [self.char2idx[char] for char in self.tokenize(text)]\n",
    "    \n",
    "    def decode(self, encoded, remove_special_tokens=False):\n",
    "        if type(encoded) == torch.Tensor:\n",
    "            encoded = encoded.tolist()\n",
    "\n",
    "        if remove_special_tokens:\n",
    "            eos_idx = encoded.index(self.eos_id) if self.eos_id in encoded else len(encoded)\n",
    "            encoded = encoded[:eos_idx]\n",
    "            encoded = [token for token in encoded if token not in self.special_tokens_id]\n",
    "\n",
    "        return \"\".join([self.idx2char[idx] for idx in encoded])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DateTokenizer(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 2, 19, 38, 39,  4, 10, 12,  5,  4, 10, 18, 18, 12,  3]), tensor([ 2, 10, 18, 18, 12,  6,  9, 13,  6, 10, 12]), tensor([10, 18, 18, 12,  6,  9, 13,  6, 10, 12,  3]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class DateDataset(Dataset):\n",
    "    def __init__(self, dates, tokenizer=None):\n",
    "        self.dates = dates\n",
    "\n",
    "        if tokenizer is None:\n",
    "            tokenizer = DateTokenizer(dates)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dates)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        date_str, target_str = self.dates[idx]\n",
    "\n",
    "        input_encoded = self.tokenizer.encode(date_str)\n",
    "\n",
    "        target_encoded = self.tokenizer.encode(target_str)\n",
    "\n",
    "        decoder_input_encoded = target_encoded[:-1]\n",
    "        decoder_target_encoded = target_encoded[1:]\n",
    "\n",
    "        return (\n",
    "            torch.tensor(input_encoded),\n",
    "            torch.tensor(decoder_input_encoded),\n",
    "            torch.tensor(decoder_target_encoded),\n",
    "        )\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        inputs, decoder_inputs, decoder_targets = zip(*batch)\n",
    "        inputs = pad_sequence(\n",
    "            inputs, batch_first=True, padding_value=self.tokenizer.pad_id\n",
    "        )\n",
    "        decoder_inputs = pad_sequence(\n",
    "            decoder_inputs, batch_first=True, padding_value=self.tokenizer.pad_id\n",
    "        )\n",
    "        decoder_targets = pad_sequence(\n",
    "            decoder_targets, batch_first=True, padding_value=self.tokenizer.pad_id\n",
    "        )\n",
    "        return inputs, decoder_inputs, decoder_targets\n",
    "\n",
    "\n",
    "# Example usage\n",
    "dataset = DateDataset(dates)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, hidden=None, cell=None):\n",
    "        if hidden is None or cell is None:\n",
    "            hidden, cell = self.init_hidden(x.shape[0])\n",
    "            hidden = hidden.to(x.device)\n",
    "            cell = cell.to(x.device)\n",
    "\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "\n",
    "        return output, hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h_0 = torch.zeros(\n",
    "            self.lstm.num_layers,\n",
    "            batch_size,\n",
    "            self.lstm.hidden_size,\n",
    "        )\n",
    "\n",
    "        c_0 = torch.zeros(\n",
    "            self.lstm.num_layers,\n",
    "            batch_size,\n",
    "            self.lstm.hidden_size,\n",
    "        )\n",
    "\n",
    "        return h_0, c_0\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, hidden_size, num_layers=num_layers, batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None, cell=None):\n",
    "        if hidden is None or cell is None:\n",
    "            hidden, cell = self.init_hidden(x.shape[0])\n",
    "            hidden = hidden.to(x.device)\n",
    "            cell = cell.to(x.device)\n",
    "\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "\n",
    "        return output, hidden, cell\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h_0 = torch.zeros(\n",
    "            self.lstm.num_layers,\n",
    "            batch_size,\n",
    "            self.lstm.hidden_size,\n",
    "        )\n",
    "\n",
    "        c_0 = torch.zeros(\n",
    "            self.lstm.num_layers,\n",
    "            batch_size,\n",
    "            self.lstm.hidden_size,\n",
    "        )\n",
    "\n",
    "        return h_0, c_0\n",
    "\n",
    "class DateLSTM(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, tokenizer, num_layers = 2, learning_rate = 0.001, teacher_forcing_probability = 0.5) -> None:\n",
    "        super(DateLSTM, self).__init__()\n",
    "\n",
    "        self.encoder = EncoderLSTM(vocab_size, embedding_dim, hidden_size, num_layers)\n",
    "        self.decoder = DecoderLSTM(vocab_size, embedding_dim, hidden_size, num_layers)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.teacher_forcing_probability = teacher_forcing_probability\n",
    "\n",
    "    def forward(self, enc_input_batch, sos_index=2, dec_input_batch=None, teacher_forcing=False, out_length=1):\n",
    "        encoder_output, encoder_hidden, encoder_cell = self.encoder(enc_input_batch)\n",
    "        batch_size = len(enc_input_batch)\n",
    "\n",
    "        if teacher_forcing:\n",
    "            decoder_output, _, _ = self.decoder(dec_input_batch, encoder_hidden, encoder_cell)\n",
    "            return decoder_output\n",
    "        else:\n",
    "            decoder_input = (torch.zeros(batch_size, 1, dtype=torch.int64) + sos_index).to(enc_input_batch.device)\n",
    "            decoder_output = torch.empty(batch_size, out_length, self.decoder.fc.out_features).to(enc_input_batch.device)\n",
    "\n",
    "            hidden = encoder_hidden\n",
    "            cell = encoder_cell\n",
    "\n",
    "            for i in range(out_length):\n",
    "                decoder_output_i, hidden, cell = self.decoder(decoder_input, hidden)\n",
    "                decoder_output[:, i:i + 1, :] = decoder_output_i\n",
    "                decoder_input = torch.argmax(decoder_output_i, dim=-1)\n",
    "\n",
    "            return decoder_output\n",
    "        \n",
    "    def forward_batch(self, batch):\n",
    "        teacher_forcing = random.random() < self.teacher_forcing_probability\n",
    "\n",
    "        inputs, decoder_inputs, decoder_targets = batch\n",
    "        outputs = self(inputs, dec_input_batch=decoder_inputs, teacher_forcing=teacher_forcing, out_length=decoder_targets.shape[1])\n",
    "\n",
    "        loss = nn.CrossEntropyLoss(ignore_index=0)(outputs.reshape(-1, outputs.size(-1)), decoder_targets.reshape(-1))\n",
    "\n",
    "        return loss\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.forward_batch(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.forward_batch(batch)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.forward_batch(batch)\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "class DateDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, dates, tokenizer = None, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.dates = dates\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        if tokenizer is None:\n",
    "            tokenizer = DateTokenizer(dates)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        train_games, val_games, test_games = random_split(self.dates, [0.8, 0.1, 0.1])\n",
    "        self.train_dataset = DateDataset(train_games, tokenizer=self.tokenizer)\n",
    "        self.val_dataset = DateDataset(val_games, tokenizer=self.tokenizer)\n",
    "        self.test_dataset = DateDataset(test_games, tokenizer=self.tokenizer)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=self.train_dataset.collate_fn)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.val_dataset.collate_fn)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, collate_fn=self.test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type        | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | encoder | EncoderLSTM | 53.8 K | train\n",
      "1 | decoder | DecoderLSTM | 55.9 K | train\n",
      "------------------------------------------------\n",
      "109 K     Trainable params\n",
      "0         Non-trainable params\n",
      "109 K     Total params\n",
      "0.439     Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/kamil/miniconda3/envs/python/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/kamil/miniconda3/envs/python/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5439f23f39b4781953d2f3417456d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3274eca967d473c8fdd3546cc41ed49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/kamil/miniconda3/envs/python/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cdcf7bace9648e3b5109b8daa4f7099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    3.7650396823883057     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   3.7650396823883057    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 3.7650396823883057}]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the data module\n",
    "tokenizer = DateTokenizer(dates)\n",
    "data_module = DateDataModule(dates, tokenizer=tokenizer)\n",
    "data_module.setup()\n",
    "\n",
    "# Initialize the model\n",
    "input_size = len(tokenizer.tokens)\n",
    "hidden_size = 128\n",
    "output_size = len(tokenizer.tokens)\n",
    "model = DateLSTM(input_size, hidden_size, output_size, tokenizer)\n",
    "\n",
    "# # Initialize the trainer\n",
    "trainer = pl.Trainer(max_epochs=10, accelerator='gpu', fast_dev_run=True)\n",
    "\n",
    "# # Train the model\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# # Test the model\n",
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
